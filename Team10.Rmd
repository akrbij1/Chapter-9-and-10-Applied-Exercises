---
title: "Team 10"
author: "Matthew Sadosuk, Akram Bijapuri, Justin Pender, Alejandro Valencia Patino"
date: "4/6/2021"
output:
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged  
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning 2 Summary Document {.tabset}

## Chapter 9

# Chapter 9 applied exercise {.tabset}

For this exercise in chapter 9 we go back and cover support vector classifiers. 
At the end of the chapter we selected applied exercise 8. This exercise features
the OJ data set, where we will be seeing how purchase price of OJ is affected by
the 17 other variables in the dataset
################################################################################

To start this problem we need to load in the two libraries: ISLR and e1071, which
will give us access to the OJ data set. To keep the results consistent we will set 
the seed. Now we will create a random sample with 800 observations from the OJ
data set and then we will split the data into a test set and training set



```{r}
rm(list = ls())

# 8. This problem involves the OJ data set which is part of the ISLR package
library(ISLR)
library(e1071)
#(a) Create a training set containing a random sample of 800
# observations, and a test set containing the remaining observations.
set.seed(5208)

train <- sample(nrow(OJ), 800)
OJ_train <- OJ[train, ]
OJ_test <- OJ[-train, ]

```

After initializing all of the parameters we can now start to construct the support
vector classifier. In the code below we use the svm function and within the function
we are predicting the purchase price onto the entire dataset,we set the kernel 
to linear, use the training dataset, and set cost = .01. Now we take the summary 
of svm_linear, and see that there are 439 support vectors that lay along the 
hyperplane. 

```{r}
#(b) Fit a support vector classifier to the training data using
# cost=0.01, with Purchase as the response and the other variables
# as predictors. Use the summary() function to produce summary
# statistics, and describe the results obtained.

svm_linear <- svm(Purchase ~ . , kernel = "linear", data = OJ_train, cost = 0.01)
summary(svm_linear)

```


In this step I created a function that would store the model, dataset, and the object
being classified.Inside the function is create a variable called confusion_matrix
that creates a table of predicted values from the linear model and the OJ_train 
dataset. Within that function i return the mse calculation that takes the values 
from the new variable confusion_matrix. Finally you concatenate the results, 
which you times 100 by the calc_error_rate that uses the stored values that were
stored in the function. 

```{r}
#(c) What are the training and test error rates?

# calculate error rate
calc_error_rate <- function(svm_model, dataset, true_classes) {
  confusion_matrix <- table(predict(svm_model, dataset), true_classes)
  return(1 - sum(diag(confusion_matrix)) / sum(confusion_matrix))
}

cat("Training Error Rate:", 100 * calc_error_rate(svm_linear, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_linear, OJ_test, OJ_test$Purchase), "%\n")

```

In this step we will tune the svm model to try to improve the accuracy of the model.
In the function we specify the model type = svm, y-variable, data = OJ, kernel = linear,
and then display range of costs from .01 to 10. After this we will look at the summary
of svm_tune to see what the best performance number was. 


```{r}
#(d) Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.
set.seed(5208)

svm_tune <- tune(svm, Purchase ~ . , data = OJ_train, kernel = "linear", 
                 ranges = list(cost = seq(0.01, 10)))
summary(svm_tune)
```


In this step we look at the summary of svm_tune and look to see what cost gives
the best performance in the model. From looking at the summary list it looked to be
a cost of 5.01 gave the best performance. So to make sure this is correct we now
use the best.parameters test method to identify the best cost. After this we
look at the training and test error rates to see how the tune we did above improved
or hurt the model accuracy

```{r}
#(e) Compute the training and test error rates using this new value for cost.
svm_linear2 <- svm(Purchase ~ . , kernel = "linear", 
                  data = OJ_train, cost = svm_tune$best.parameters)

cat("Training Error Rate:", 100 * calc_error_rate(svm_linear2, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_linear2, OJ_test, OJ_test$Purchase), "%\n")

```


For steps f and g we will follow the same steps that were done in b to e. The only
input being changed in the model is the kernel which will be set to radial in f
and poly for the second one. At the end I will compare the test and train error rate
results and publish which model will be the best


In part c and e we are still using the function that was made above, but the only
part we will be changing is the name of the model to the new svm variable. 

```{r}
#(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.
# part b: fitting the support vector 
set.seed(5208)
svm_radial <- svm(Purchase ~ . , data = OJ_train, kernel = "radial")
summary(svm_radial)

# part c: Calculating the test and error rate
cat("Training Error Rate:", 100 * calc_error_rate(svm_radial, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_radial, OJ_test, OJ_test$Purchase), "%\n")

# part d: Adding the costs from .01 to 10
set.seed(5208)
svm_tune2 <- tune(svm, Purchase ~ . , data = OJ_train, kernel = "radial",
                 ranges = list(cost = seq(0.01, 10)))
summary(svm_tune)

# part e: Take the best performance from part d and set that equal to the cost input
svm_radial2 <- svm(Purchase ~ . , data = OJ_train, kernel = "radial",
                  cost = svm_tune$best.parameters)

cat("Training Error Rate:", 100 * calc_error_rate(svm_radial2, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_radial2, OJ_test, OJ_test$Purchase), "%\n")

```





```{r}
#(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree=2.

# part b: fitting the support vector 
set.seed(5208)

svm_poly <- svm(Purchase ~ . , data = OJ_train, kernel = "poly", degree = 2)
summary(svm_poly)

# part c: Calculating the test and error rate
cat("Training Error Rate:", 100 * calc_error_rate(svm_poly, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_poly, OJ_test, OJ_test$Purchase), "%\n")

# part d: Adding the costs from .01 to 10

set.seed(5208)
svm_tune3 <- tune(svm, Purchase ~ . , data = OJ_train, kernel = "poly", 
                 degree = 2, ranges = list(cost = seq(0.01, 10)))
summary(svm_tune)

# part e: Take the best performance from part d and set that equal to the cost input

svm_poly2 <- svm(Purchase ~ . , data = OJ_train, kernel = "poly", 
                degree = 2, cost = svm_tune$best.parameters$cost)

cat("Training Error Rate:", 100 * calc_error_rate(svm_poly2, OJ_train, OJ_train$Purchase), "%\n")
cat("Test Error Rate:", 100 * calc_error_rate(svm_poly2, OJ_test, OJ_test$Purchase), "%\n")
```


(h) Overall, which approach seems to give the best results on this data?

Overall, radial basis kernel seems to be producing minimum misclassification 
error on training set but the linear kernel performs better on test data.


## Chapter 10

# Chapter 10 applied exercise {.tabset}

"""
(a) Generate a simulated data set with 20 observations in each of
three classes (i.e. 60 observations total), and 50 variables.
Hint: There are a number of functions in R that you can use to
generate data. One example is the rnorm() function; runif() is
another option. Be sure to add a mean shift to the observations
in each class so that there are three distinct classes.
"""

```{r}
#(a) Generate a simulated data set with 20 observations in each of three classes 
set.seed(11)

class1 <- matrix(rnorm(20*50, mean = -3), nrow=20) #simulated random data
class2 <- matrix(rnorm(20*50, mean = 0), nrow=20) #simulated random data
class3 <- matrix(rnorm(20*50, mean = 3), nrow=20) #simulated random data

df <- data.frame(rbind(class1,class2,class3))

mean(rowMeans(df)[1:20])
mean(rowMeans(df)[21:40])
mean(rowMeans(df)[41:60])

plot(df$X1, main="Do you see any possible clusters in variable 1?",
     xlab ="Row Index", ylab="df$X1 Value", pch =20, cex =2)
plot(df$X2, main="Do you see any possible clusters in variable 2?",
     xlab ="Row Index", ylab="df$X2 Value", pch =20, cex =2)
plot(df$X29, main="Do you see any possible clusters in variable 29?",
     xlab ="Row Index", ylab="df$X29 Value", pch =20, cex =2)
plot(df$X49, main="Do you see any possible clusters in variable 49?",
     xlab ="Row Index", ylab="df$X49 Value", pch =20, cex =2)


```

(b) Perform PCA on the 60 observations and plot the first two principal
component score vectors. Use a different color to indicate
the observations in each of the three classes. If the three classes
appear separated in this plot, then continue on to part (c). If
not, then return to part (a) and modify the simulation so that
there is greater separation between the three classes. Do not
continue to part (c) until the three classes show at least some
separation in the first two principal component score vectors.


```{r}
set.seed(11)


```

(c) Perform K-means clustering of the observations with K = 3.
How well do the clusters that you obtained in K-means clustering
compare to the true class labels?
Hint: You can use the table() function in R to compare the true
class labels to the class labels obtained by clustering. Be careful
how you interpret the results: K-means clustering will arbitrarily
number the clusters, so you cannot simply check whether the true
class labels and clustering labels are the same.
```{r}
set.seed(11)
Kclasses = c(rep(1,20), rep(2,20), rep(3,20))


km.out3 <- kmeans(df,3,nstart=50)
km.out3$cluster

table(km.out3$cluster, Kclasses, dnn=c("Clusters","Class Labels"))

plot(df$X1, col =(km.out3$cluster +1) , main="K-Means Clustering
Results with K=3", xlab ="Row Index", ylab="df$X1 Value", pch =20, cex =2)

plot(df$X2, col =(km.out3$cluster +1) , main="K-Means Clustering
Results with K=3", xlab ="Row Index", ylab="df$X2 Value", pch =20, cex =2)

plot(df$X29, col =(km.out3$cluster +1) , main="K-Means Clustering
Results with K=3", xlab ="Row Index", ylab="df$X29 Value", pch =20, cex =2)

plot(df$X49, col =(km.out3$cluster +1) , main="K-Means Clustering
Results with K=3", xlab ="Row Index", ylab="df$X49 Value", pch =20, cex =2)

```

Perform K-means clustering with K = 2. Describe your results.
```{r}
set.seed(99)
km.out2 <- kmeans(df,2,nstart=50)
km.out2$cluster

table(km.out2$cluster, Kclasses, dnn=c("Clusters","Class Labels"))

plot(df$X1, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="Row Index", ylab="df$X1 Value", pch =20, cex =2)

plot(df$X2, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="Row Index", ylab="df$X2 Value", pch =20, cex =2)

plot(df$X29, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="Row Index", ylab="df$X29 Value", pch =20, cex =2)

plot(df$X49, col =(km.out2$cluster +1) , main="K-Means Clustering
Results with K=2", xlab ="Row Index", ylab="df$X49 Value", pch =20, cex =2)

```

Now perform K-means clustering with K = 4, and describe your
results.
```{r}
set.seed(10)
km.out4 <- kmeans(df,4,nstart=50)
km.out4$cluster

table(km.out4$cluster, Kclasses, dnn=c("Clusters","Class Labels"))

plot(df$X1, col =(km.out4$cluster +1) , main="K-Means Clustering
Results with K=4", xlab ="Row Index", ylab="df$X1 Value", pch =20, cex =2)

plot(df$X2, col =(km.out4$cluster +1) , main="K-Means Clustering
Results with K=4", xlab ="Row Index", ylab="df$X2 Value", pch =20, cex =2)

plot(df$X29, col =(km.out4$cluster +1) , main="K-Means Clustering
Results with K=4", xlab ="Row Index", ylab="df$X29 Value", pch =20, cex =2)

plot(df$X49, col =(km.out4$cluster +1) , main="K-Means Clustering
Results with K=4", xlab ="Row Index", ylab="df$X49 Value", pch =20, cex =2)


```

Now perform K-means clustering with K = 3 on the first two
principal component score vectors, rather than on the raw data.
That is, perform K-means clustering on the 60 × 2 matrix of
which the first column is the first principal component score
vector, and the second column is the second principal.
```{r}



```

Using the scale() function, perform K-means clustering with
K = 3 on the data after scaling each variable to have standard
deviation one. How do these results compare to those obtained
in (b)? Explain.
```{r}
set.seed(11)

scaled_df <- data.frame(scale(df))


print(sd(scaled_df$X1))
print(sd(scaled_df$X2))
print(sd(scaled_df$X29))
print(sd(scaled_df$X39))

km.out3scaled = kmeans(scaled_df,3,nstart=50) #scale
km.out3scaled$cluster


table(km.out3scaled$cluster, Kclasses, dnn=c("Clusters","Class Labels"))

plot(df$X1, col =(km.out3scaled$cluster +1) , main="K-Means Clustering
Results with K=3 and Scaled  (SD = 1)", xlab ="df$X1 Value", ylab="", pch =20, cex =2)



plot(df$X2, col =(km.out3scaled$cluster +1) , main="K-Means Clustering
Results with K=3 and Scaled  (SD = 1)", xlab ="df$X2 Value", ylab="", pch =20, cex =2)


plot(df$X29, col =(km.out3scaled$cluster +1) , main="K-Means Clustering
Results with K=3 and Scaled  (SD = 1)", xlab ="df$X29 Value", ylab="", pch =20, cex =2)


plot(df$X49, col =(km.out3scaled$cluster +1) , main="K-Means Clustering
Results with K=3 and Scaled  (SD = 1)", xlab ="df$X49 Value", ylab="", pch =20, cex =2)

```

